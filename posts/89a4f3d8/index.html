<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>现代自然语言处理最佳、最新进展 | 路双宁的博客</title><meta name="keywords" content="翻译,NLP,博客"><meta name="author" content="路双宁"><meta name="copyright" content="路双宁"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><meta name="description" content="博客原文：The Best and Most Current of Modern Natural Language Processing 作者：Victor Sanh   Photo by Radu Marcusu on Unsplash  过去的两年，NLP社区目睹了各种任务和应用的加速发展🚀。这一进展是由于我们传统地构建NLP系统的方式发生了转变：长久以来，我们使用预训练的词嵌入，像word">
<meta property="og:type" content="article">
<meta property="og:title" content="现代自然语言处理最佳、最新进展">
<meta property="og:url" content="https://lushuangning.github.io/posts/89a4f3d8/index.html">
<meta property="og:site_name" content="路双宁的博客">
<meta property="og:description" content="博客原文：The Best and Most Current of Modern Natural Language Processing 作者：Victor Sanh   Photo by Radu Marcusu on Unsplash  过去的两年，NLP社区目睹了各种任务和应用的加速发展🚀。这一进展是由于我们传统地构建NLP系统的方式发生了转变：长久以来，我们使用预训练的词嵌入，像word">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg">
<meta property="article:published_time" content="2019-05-23T14:56:28.000Z">
<meta property="article:modified_time" content="2024-12-26T05:09:44.184Z">
<meta property="article:author" content="路双宁">
<meta property="article:tag" content="翻译">
<meta property="article:tag" content="NLP">
<meta property="article:tag" content="博客">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://lushuangning.github.io/posts/89a4f3d8/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.10.5/dist/instantsearch.min.css"><script src="https://cdn.jsdelivr.net/npm/instantsearch.js@2.10.5/dist/instantsearch.min.js" defer></script><script>var GLOBAL_CONFIG = { 
  root: '/',
  hexoversion: '5.4.2',
  algolia: {"appId":"NT7VBMEHNU","apiKey":"db90b9537944fac58a60e68f42c776cf","indexName":"hexo","hits":{"per_page":6},"languages":{"input_placeholder":"搜索文章","hits_empty":"找不到您查询的内容：${query}","hits_stats":"找到 ${hits} 条结果，用时 ${time} 毫秒"}},
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"limitCount":100,"languages":{"author":"作者: 路双宁","link":"链接: ","source":"来源: 路双宁的博客","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  ClickShowText: {"text":"图灵,麦卡锡,明斯基,冯·诺依曼,伯纳斯·李,香农,肯·汤普森,林纳斯·托瓦兹","fontSize":"15px"},
  lightbox: 'mediumZoom',
  Snackbar: undefined,
  justifiedGallery: {
    js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
    css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
};

var saveToLocal = {
  set: function setWithExpiry(key, value, ttl) {
    const now = new Date()
    const expiryDay = ttl * 86400000
    const item = {
      value: value,
      expiry: now.getTime() + expiryDay,
    }
    localStorage.setItem(key, JSON.stringify(item))
  },

  get: function getWithExpiry(key) {
    const itemStr = localStorage.getItem(key)

    if (!itemStr) {
      return undefined
    }
    const item = JSON.parse(itemStr)
    const now = new Date()

    if (now.getTime() > item.expiry) {
      localStorage.removeItem(key)
      return undefined
    }
    return item.value
  }
}</script><script id="config_change">var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isSidebar: true,
  postUpdate: '2024-12-26 13:09:44'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(function () {
  window.activateDarkMode = function () {
    document.documentElement.setAttribute('data-theme', 'dark')
    if (document.querySelector('meta[name="theme-color"]') !== null) {
      document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
    }
  }
  window.activateLightMode = function () {
    document.documentElement.setAttribute('data-theme', 'light')
    if (document.querySelector('meta[name="theme-color"]') !== null) {
      document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
    }
  }

  const autoChangeMode = 'true'
  const t = saveToLocal.get('theme')
  if (autoChangeMode === '1') {
    const isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
    const isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
    const isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
    const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

    if (t === undefined) {
      if (isLightMode) activateLightMode()
      else if (isDarkMode) activateDarkMode()
      else if (isNotSpecified || hasNoSupport) {
        const now = new Date()
        const hour = now.getHours()
        const isNight = hour <= 6 || hour >= 18
        isNight ? activateDarkMode() : activateLightMode()
      }
      window.matchMedia('(prefers-color-scheme: dark)').addListener(function (e) {
        if (saveToLocal.get('theme') === undefined) {
          e.matches ? activateDarkMode() : activateLightMode()
        }
      })
    } else if (t === 'light') activateLightMode()
    else activateDarkMode()
  } else if (autoChangeMode === '2') {
    const now = new Date()
    const hour = now.getHours()
    const isNight = hour <= 6 || hour >= 18
    if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
    else if (t === 'light') activateLightMode()
    else activateDarkMode()
  } else {
    if (t === 'dark') activateDarkMode()
    else if (t === 'light') activateLightMode()
  }
})()</script><style type="text/css">#toggle-sidebar {bottom: 80px}</style><link rel="stylesheet" href="//at.alicdn.com/t/font_2149337_g9rq691k4pp.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome/css/font-awesome.min.css"><script src="https://cdn.jsdelivr.net/gh/lushuangning/live2d-widget@latest/"></script><meta name="generator" content="Hexo 5.4.2"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="avatar-img" src="/img/avatar.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="mobile_post_data"><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">文章</div><div class="length_num">105</div></a></div></div><div class="mobile_data_item is-center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">标签</div><div class="length_num">58</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/gallery/"><i class="fa-fw fas fa-images"></i><span> 画廊</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div id="body-wrap"><div id="sidebar"><i class="fas fa-arrow-right on" id="toggle-sidebar"></i><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8D%9A%E5%AE%A2%E5%8E%9F%E6%96%87%EF%BC%9AThe-Best-and-Most-Current-of-Modern-Natural-Language-Processing"><span class="toc-number">1.</span> <span class="toc-text">博客原文：The Best and Most Current of Modern Natural Language Processing</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%BD%9C%E8%80%85%EF%BC%9AVictor-Sanh"><span class="toc-number">1.1.</span> <span class="toc-text">作者：Victor Sanh</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%F0%9F%8C%8A-%E4%B8%80%E7%A7%8D%E6%96%B0%E8%8C%83%E5%BC%8F%EF%BC%9A%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0"><span class="toc-number"></span> <span class="toc-text">🌊 一种新范式：迁移学习</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%F0%9F%96%BC-%E8%A1%A8%E7%A4%BA%E5%AD%A6%E4%B9%A0"><span class="toc-number"></span> <span class="toc-text">🖼 表示学习</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%F0%9F%97%A3-%E7%A5%9E%E7%BB%8F%E5%AF%B9%E8%AF%9D"><span class="toc-number"></span> <span class="toc-text">🗣 神经对话</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%F0%9F%8D%B1-%E4%BB%BB%E4%BD%A0%E9%80%89"><span class="toc-number"></span> <span class="toc-text">🍱 任你选</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%F0%9F%8C%8D-%E9%80%9A%E7%94%A8%E8%B5%84%E6%BA%90"><span class="toc-number"></span> <span class="toc-text">🌍 通用资源</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B9%A6%E7%B1%8D%EF%BC%9A"><span class="toc-number"></span> <span class="toc-text">书籍：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AF%BE%E7%A8%8B%E6%9D%90%E6%96%99%EF%BC%9A"><span class="toc-number"></span> <span class="toc-text">课程材料：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8D%9A%E5%AE%A2%E5%92%8C%E6%92%AD%E5%AE%A2"><span class="toc-number"></span> <span class="toc-text">博客和播客</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%B6%E4%BB%96"><span class="toc-number"></span> <span class="toc-text">其他</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%F0%9F%8E%85-%E5%86%99%E5%9C%A8%E6%9C%80%E5%90%8E"><span class="toc-number"></span> <span class="toc-text">🎅 写在最后</span></a></div></div></div><header class="no-top-img" id="page-header"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">路双宁的博客</a></span><span id="menus"><div id="search_button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/gallery/"><i class="fa-fw fas fa-images"></i><span> 画廊</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><span class="close" id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></span></span></nav></header><main class="layout_post" id="content-inner"><article id="post"><div id="post-info"><div id="post-title"><div class="posttitle">现代自然语言处理最佳、最新进展</div></div><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2019-05-23T14:56:28.000Z" title="发表于 2019-05-23 22:56:28">2019-05-23</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-12-26T05:09:44.184Z" title="更新于 2024-12-26 13:09:44">2024-12-26</time></span></div><div class="meta-secondline"> <span class="post-meta-separator">|</span><span class="post-meta-pv-cv"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div><div class="post-content" id="article-container"><h4 id="博客原文：The-Best-and-Most-Current-of-Modern-Natural-Language-Processing">博客原文：<a target="_blank" rel="noopener external nofollow noreferrer" href="https://medium.com/huggingface/the-best-and-most-current-of-modern-natural-language-processing-5055f409a1d1">The Best and Most Current of Modern Natural Language Processing</a></h4>
<h5 id="作者：Victor-Sanh">作者：Victor Sanh</h5>
<hr>
<p><img src="https://lushuangning.oss-cn-beijing.aliyuncs.com/image/201905231.jpg" alt=""></p>
<center>Photo by Radu Marcusu on Unsplash</center>
<br/>
<p>过去的两年，NLP社区目睹了各种任务和应用的加速发展🚀。这一进展是由于我们传统地构建NLP系统的方式发生了转变：长久以来，我们使用预训练的词嵌入，像word2vec或GloVe来初始化神经网络的第一层，然后用单个数据集通过监督学习来训练具体任务的体系结构。</p>
<p>最近，一些作品证明了我们可以在<strong>网络规模的数据集📖</strong>上学习分层上下文表示，利用<strong>无监督（或半监督）信号例如语言模型</strong>，把这些预训练任务的转换成下游任务（迁移学习）。令人鼓舞地是，这种转换导致了各种下游应用的重大进步，从问答，到自然语言推理，再到的句法分析。</p>
<span id="more"></span>
<blockquote>
<p>我该读哪些论文来了解现代NLP的最新趋势？</p>
</blockquote>
<p>几周前，我的一个朋友决定潜心研究NLP。他已经有机器学习和深度学习的背景，所以他真诚地问我：<strong>“我该读哪些论文来了解现代NLP的最新趋势？”。</strong> 👩‍🎓👨‍🎓</p>
<p>这是一个好问题，尤其是当你考虑到NLP会议（和一般的机器学习会议）获得指数增长的提交数量时，2019年NAACL的提交量比2018年增加了<strong>80%</strong>，ACL增加了<strong>90%</strong>。</p>
<p>我为他编写了这个论文<strong>列表和资源</strong>📚，并且我认为把它分享给NLP社区会很棒，我相信它会帮到更多人。</p>
<p><strong>免责声明</strong>：这个列表<strong>不会面面俱到</strong>，<strong>也不会囊括NLP的每一个领域</strong>（例如，没有语义分析，对抗学习，增强学习在NLP方面的应用）。这是过去几年、几个月中最新最有影响的作品（截至2019年5月），主要是受到<strong>我</strong>所读的东西的影响。</p>
<p>通常来说，一个好的开始方式是读介绍性或总结性的博客文章（例如，<a target="_blank" rel="noopener external nofollow noreferrer" href="http://mostafadehghani.com/2019/05/05/universal-transformers/">这篇文章</a>或<a target="_blank" rel="noopener external nofollow noreferrer" href="https://openai.com/blog/better-language-models/">这篇</a>），在真正花费时间读论文之前，这些文章能从高层次的视角给你足够的背景 ✋。</p>
<p><img src="https://lushuangning.oss-cn-beijing.aliyuncs.com/image/201905232.jpeg?imageMogr2/thumbnail/!75p/" alt=""></p>
<center>Who said that naming models should be boring and sad? — Source: Moviefone</center>
<h2 id="🌊-一种新范式：迁移学习">🌊 一种新范式：迁移学习</h2>
<ul>
<li>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://arxiv.org/abs/1802.05365">Deep contextualized word representations</a>(NAACL 2018)<br>
Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, Luke Zettlemoyer<br>
<br/></p>
</li>
<li>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://arxiv.org/abs/1801.06146">Universal Language Model Fine-tuning for Text Classification</a>(ACL 2018)<br>
Jeremy Howard, Sebastian Ruder<br>
<br/></p>
</li>
<li>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf">Improving Language Understanding by Generative Pre-Training</a><br>
Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever<br>
<br/></p>
</li>
<li>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">Language Models are Unsupervised Multitask Learners</a><br>
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever<br>
<br/></p>
</li>
<li>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://arxiv.org/abs/1810.04805">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a>(NAACL 2019)<br>
Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova<br>
<br/></p>
</li>
<li>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://arxiv.org/abs/1903.07785">Cloze-driven Pretraining of Self-attention Networks</a>(arXiv 2019)<br>
Alexei Baevski, Sergey Edunov, Yinhan Liu, Luke Zettlemoyer, Michael Auli<br>
<br/></p>
</li>
<li>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://arxiv.org/abs/1905.03197">Unified Language Model Pre-training for Natural Language Understanding and Generation</a>(arXiv 2019)<br>
Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, Hsiao-Wuen Hon<br>
<br/></p>
</li>
<li>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://arxiv.org/abs/1905.02450">MASS: Masked Sequence to Sequence Pre-training for Language Generation</a>(ICML 2019)<br>
Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, Tie-Yan Liu<br>
<br/></p>
</li>
</ul>
<p><img src="https://lushuangning.oss-cn-beijing.aliyuncs.com/image/201905233.png?imageMogr2/thumbnail/!75p/" alt=""></p>
<center>The Transformer architecture has become ubiquitous in sequence modeling tasks. — Source: </center>
<center><a target="_blank" rel="noopener external nofollow noreferrer" href="https://arxiv.org/abs/1706.03762">Attention is all you need</a></center>
<br/>
<h2 id="🖼-表示学习">🖼 表示学习</h2>
<ul>
<li>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://arxiv.org/abs/1805.01070">What you can cram into a single vector: Probing sentence embeddings for linguistic properties</a>(ACL 2018)<br>
Alexis Conneau, German Kruszewski, Guillaume Lample, Loïc Barrault, Marco Baroni<br>
<br/></p>
</li>
<li>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://arxiv.org/abs/1901.10444">No Training Required: Exploring Random Encoders for Sentence Classification</a>(ICLR 2019)<br>
John Wieting, Douwe Kiela<br>
<br/></p>
</li>
<li>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://arxiv.org/abs/1804.07461">GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding</a>(ICLR 2019)<br>
Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel R. Bowman<br>
和<br>
<a target="_blank" rel="noopener external nofollow noreferrer" href="https://arxiv.org/abs/1905.00537">SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems</a>(arXiv 2019)<br>
Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel R. Bowman<br>
<br/></p>
</li>
<li>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://arxiv.org/abs/1903.08855">Linguistic Knowledge and Transferability of Contextual Representations</a>(NAACL 2019)<br>
Nelson F. Liu, Matt Gardner, Yonatan Belinkov, Matthew E. Peters, Noah A. Smith<br>
<br/></p>
</li>
<li>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://arxiv.org/abs/1903.05987">To Tune or Not to Tune? Adapting Pretrained Representations to Diverse Tasks</a>(arXiv 2019)<br>
Matthew Peters, Sebastian Ruder, Noah A. Smith<br>
<br/></p>
</li>
</ul>
<h2 id="🗣-神经对话">🗣 神经对话</h2>
<ul>
<li>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://arxiv.org/abs/1506.05869">A Neural Conversational Model</a>(ICML Deep Learning Workshop 2015)<br>
Oriol Vinyals, Quoc Le<br>
<br/></p>
</li>
<li>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://arxiv.org/abs/1603.06155">A Persona-Based Neural Conversation Model</a>(ACL 2016)<br>
Jiwei Li, Michel Galley, Chris Brockett, Georgios P. Spithourakis, Jianfeng Gao, Bill Dolan<br>
<br/></p>
</li>
<li>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://arxiv.org/abs/1611.08562">A Simple, Fast Diverse Decoding Algorithm for Neural Generation</a>(arXiv 2017)<br>
Jiwei Li, Will Monroe, Dan Jurafsky<br>
<br/></p>
</li>
<li>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://arxiv.org/abs/1809.08267">Neural Approaches to Conversational AI</a>(arXiv 2018)<br>
Jianfeng Gao, Michel Galley, Lihong Li<br>
<br/></p>
</li>
<li>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://arxiv.org/abs/1901.08149">TransferTransfo: A Transfer Learning Approach for Neural Network Based Conversational Agents</a>(NeurIPS 2018 CAI Workshop)<br>
Thomas Wolf, Victor Sanh, Julien Chaumond, Clement Delangue<br>
免责声明：我是这个出版物的作者之一<br>
<a target="_blank" rel="noopener external nofollow noreferrer" href="https://medium.com/huggingface/how-to-build-a-state-of-the-art-conversational-ai-with-transfer-learning-2d818ac26313">逐步解释博客文章</a><br>
<br/></p>
</li>
<li>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://arxiv.org/abs/1811.01241">Wizard of Wikipedia: Knowledge-Powered Conversational agents</a>(ICLR 2019)<br>
Emily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, Jason Weston<br>
<br/></p>
</li>
<li>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://arxiv.org/abs/1903.03094">Learning to Speak and Act in a Fantasy Text Adventure Game</a>(arXiv 2019)<br>
Jack Urbanek, Angela Fan, Siddharth Karamcheti, Saachi Jain, Samuel Humeau, Emily Dinan, Tim Rocktäschel, Douwe Kiela, Arthur Szlam, Jason Weston<br>
<br/></p>
</li>
</ul>
<h2 id="🍱-任你选">🍱 任你选</h2>
<ul>
<li>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://arxiv.org/abs/1506.03134">Pointer Networks</a>(NIPS 2015)<br>
Oriol Vinyals, Meire Fortunato, Navdeep Jaitly<br>
<br/></p>
</li>
<li>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://arxiv.org/abs/1503.08895">End-To-End Memory Networks</a>(NIPS 2015)<br>
Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, Rob Fergus<br>
<br/></p>
</li>
<li>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://arxiv.org/abs/1704.04368">Get To The Point: Summarization with Pointer-Generator Networks</a>(ACL 2017)<br>
Abigail See, Peter J. Liu, Christopher D. Manning<br>
<br/></p>
</li>
<li>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://arxiv.org/abs/1705.02364">Supervised Learning of Universal Sentence Representations from Natural Language Inference Data</a>(EMNLP 2017)<br>
Alexis Conneau, Douwe Kiela, Holger Schwenk, Loic Barrault, Antoine Bordes<br>
<br/></p>
</li>
<li>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://arxiv.org/abs/1707.07045">End-to-end Neural Coreference Resolution</a>(EMNLP 2017)<br>
Kenton Lee, Luheng He, Mike Lewis, Luke Zettlemoyer<br>
<br/></p>
</li>
<li>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://arxiv.org/abs/1709.03856">StarSpace: Embed All The Things!</a>(AAAI 2018)<br>
Ledell Wu, Adam Fisch, Sumit Chopra, Keith Adams, Antoine Bordes, Jason Weston<br>
<br/></p>
</li>
<li>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://arxiv.org/abs/1806.08730">The Natural Language Decathlon: Multitask Learning as Question Answering</a>(arXiv 2018)<br>
Bryan McCann, Nitish Shirish Keskar, Caiming Xiong, Richard Socher<br>
<br/></p>
</li>
<li>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://arxiv.org/abs/1808.04444">Character-Level Language Modeling with Deeper Self-Attention</a>(arXiv 2018)<br>
Rami Al-Rfou, Dokook Choe, Noah Constant, Mandy Guo, Llion Jones<br>
<br/></p>
</li>
<li>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://arxiv.org/abs/1804.08199">Linguistically-Informed Self-Attention for Semantic Role Labeling</a>(EMNLP 2018)<br>
Emma Strubell, Patrick Verga, Daniel Andor, David Weiss, Andrew McCallum<br>
<br/></p>
</li>
<li>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://arxiv.org/abs/1804.07755">Phrase-Based &amp; Neural Unsupervised Machine Translation</a>(EMNLP 2018)<br>
Guillaume Lample, Myle Ott, Alexis Conneau, Ludovic Denoyer, Marc’Aurelio Ranzato<br>
<br/></p>
</li>
<li>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://arxiv.org/abs/1804.00079">Learning General Purpose Distributed Sentence Representations via Large Scale Multi-task Learning</a>(ICLR 2018)<br>
Sandeep Subramanian, Adam Trischler, Yoshua Bengio, Christopher J Pal<br>
<br/></p>
</li>
<li>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://arxiv.org/abs/1901.02860">Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context</a>(arXiv 2019)<br>
Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V. Le, Ruslan Salakhutdinov<br>
<br/></p>
</li>
<li>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://arxiv.org/abs/1807.03819">Universal Transformers</a>(ICLR 2019)<br>
Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, Łukasz Kaiser<br>
<br/></p>
</li>
<li>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://arxiv.org/abs/1902.10547">An Embarrassingly Simple Approach for Transfer Learning from Pretrained Language Models</a>(NAACL 2019)<br>
Alexandra Chronopoulou, Christos Baziotis, Alexandros Potamianos<br>
<br/></p>
</li>
<li>
<p>…对于老一点的论文，在选择阅读内容时，引用量通常是一个合理的参考。<br>
<br/></p>
</li>
</ul>
<p>一个好的法则是，你应该阅读那些你感兴趣并能激发你快乐的文章！🤷‍🌟</p>
<h2 id="🌍-通用资源">🌍 通用资源</h2>
<p>还有大量可选择的资源供你使用，并不一定是论文，如下所示：</p>
<h3 id="书籍：">书籍：</h3>
<ul>
<li>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://web.stanford.edu/~jurafsky/slp3/">Speech and Language Processing</a>(3rd ed. draft)<br>
Dan Jurafsky and James H. Martin<br>
<br/></p>
</li>
<li>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://www.morganclaypool.com/doi/abs/10.2200/S00762ED1V01Y201703HLT037">Neural Network Methods for Natural Language Processing</a><br>
Yoav Goldberg<br>
<br/></p>
</li>
</ul>
<h3 id="课程材料：">课程材料：</h3>
<ul>
<li>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://docs.google.com/document/d/1kXhxA4iit2fhAJJGOb32bb151cKLJtW8xWuyMVwqD6s/edit">Natural Language Understanding and Computational Semantics</a> with Katharina Kann and Sam Bowman at NYU<br>
<br/></p>
</li>
<li>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="http://web.stanford.edu/class/cs224n/">CS224n: Natural Language Processing with Deep Learning</a> with Chris Manning and Abigail See at Standford<br>
<br/></p>
</li>
<li>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://arxiv.org/pdf/1902.06006.pdf">Contextual Word Representations: A Contextual Introduction</a> from Noah A. Smith’s teaching material at UW<br>
<br/></p>
</li>
</ul>
<h3 id="博客和播客">博客和播客</h3>
<ul>
<li>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="http://ruder.io/">Sebastian Ruder’s blog</a><br>
<br/></p>
</li>
<li>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="http://jalammar.github.io/">Jay Alammar’s illustrated blog</a><br>
<br/></p>
</li>
<li>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://podcasts.apple.com/us/podcast/nlp-highlights/id1235937471">NLP Highlights</a> hosted by Matt Gardner and Waleed Ammar<br>
<br/></p>
</li>
</ul>
<h3 id="其他">其他</h3>
<ul>
<li>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://paperswithcode.com/">Papers With Code</a><br>
<br/></p>
</li>
<li>
<p>Twitter🐦<br>
<br/></p>
</li>
<li>
<p>arXiv daily newsletter<br>
<br/></p>
</li>
<li>
<p>Survey papers<br>
<br/></p>
</li>
<li>
<p>…<br>
<br/></p>
</li>
</ul>
<h2 id="🎅-写在最后">🎅 写在最后</h2>
<p>就到这里！阅读其中的这些资源应该可以让你对现代NLP的最新趋势有一个很好的认识并希望帮你建立自己的NLP系统！🎮</p>
<p>最后一件事，我没有在这篇博客里过多谈及，但是我发现它是极其重要的（有时候可以忽略），那就是<strong>动手实践比单纯的阅读要更好！</strong> 👩‍💻通过深入阅读附带的代码或尝试自己实现一些代码，你常常能学到更多。实践的资源包括<a target="_blank" rel="noopener external nofollow noreferrer" href="http://fast.ai/">the amazing blog posts and courses from fast.ai</a>或我们的<a target="_blank" rel="noopener external nofollow noreferrer" href="https://github.com/huggingface">开源库</a>🤗</p>
<p>你感觉如何呢？哪些作品对你影响最深？现在就告诉我们吧！⌨️</p>
<p>像往常一样，如果你喜欢这篇文章，👏 告诉我们并分享一些你身边的消息吧！</p>
<p>非常感谢Lysandre Debut, Clément Delangue, Thibault Févry, Peter Martigny, Anthony Moi and Thomas Wolf 提供的评价和反馈。</p>
</div><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined" rel="external nofollow noreferrer">路双宁</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://lushuangning.github.io/posts/89a4f3d8/">https://lushuangning.github.io/posts/89a4f3d8/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="external nofollow noreferrer" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://lushuangning.github.io" target="_blank">路双宁的博客</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E7%BF%BB%E8%AF%91/">翻译</a><a class="post-meta__tags" href="/tags/NLP/">NLP</a><a class="post-meta__tags" href="/tags/%E5%8D%9A%E5%AE%A2/">博客</a></div><div class="post_share"><div class="social-share" data-image="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i> 打赏<div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="/img/wechatpay.jpg" target="_blank"><img class="post-qr-code-img" src="/img/wechatpay.jpg" alt="微信"/></a><div class="post-qr-code-desc">微信</div></li><li class="reward-item"><a href="/img/alipay.jpg" target="_blank"><img class="post-qr-code-img" src="/img/alipay.jpg" alt="支付宝"/></a><div class="post-qr-code-desc">支付宝</div></li></ul></div></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/posts/793a84b3/"><img class="prev-cover" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Universal Transformers</div></div></a></div><div class="next-post pull-right"><a href="/posts/5594da42/"><img class="next-cover" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">性代数回顾及参考——矩阵积分</div></div></a></div></nav><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div></div></div></article></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2018 - 2024 By 路双宁</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener external nofollow noreferrer" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener external nofollow noreferrer" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><section id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></section><div class="search-dialog" id="algolia-search"><div class="search-dialog__title" id="algolia-search-title">Algolia</div><div id="algolia-input-panel"><div id="algolia-search-input"></div></div><hr/><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-stats"></div></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div><div><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/medium-zoom/dist/medium-zoom.min.js"></script><script src="/js/search/algolia.js"></script><script>var preloader = {
  endLoading: () => {
    document.body.style.overflow = 'auto';
    document.getElementById('loading-box').classList.add("loaded")
  },
  initLoading: () => {
    document.body.style.overflow = '';
    document.getElementById('loading-box').classList.remove("loaded")

  }
}
window.addEventListener('load',()=> {preloader.endLoading()})</script><div class="js-pjax"><script>if (document.getElementsByClassName('mermaid').length) {
  if (window.mermaidJsLoad) mermaid.init()
  else {
    $.getScript('https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js', function () {
      window.mermaidJsLoad = true
      mermaid.initialize({
        theme: 'default',
      })
      true && mermaid.init()
    })
  }
}</script><script>function loadValine () {
  function initValine () {
    const initData = {
      el: '#vcomment',
      appId: 'VMqnI9pLmxxHTgJswVD4NG7b-gzGzoHsz',
      appKey: 'IIEWxiiBotI5HSnW0J416lsU',
      placeholder: '快来评论吧',
      avatar: 'monsterid',
      meta: 'nick,mail,link'.split(','),
      pageSize: '10',
      lang: 'en',
      recordIP: false,
      serverURLs: '',
      emojiCDN: '',
      emojiMaps: "",
      enableQQ: false,
      path: window.location.pathname,
    }

    if (true) { 
      initData.requiredFields= ('nick,mail'.split(','))
    }

    const valine = new Valine(initData)
  }

  if (typeof Valine === 'function') initValine() 
  else $.getScript('https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js', initValine)
}

if ('Valine' === 'Valine' || !true) {
  if (true) btf.loadComment(document.querySelector('#vcomment'),loadValine)
  else setTimeout(() => loadValine(), 0)
} else {
  function loadOtherComment () {
    loadValine()
  }
}</script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><div class="aplayer no-destroy" data-id="003B4aet4U1Flz" data-server="tencent" data-type="song" data-fixed="true" data-mini="true" data-listFolded="false" data-order="random" data-preload="none" data-autoplay="false" muted></div><script id="click-show-text" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/click-show-text.min.js" async="async" mobile="false"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js"></script><script src="https://cdn.jsdelivr.net/gh/metowolf/MetingJS@1.2/dist/Meting.min.js"></script><script src="https://cdn.jsdelivr.net/npm/pjax/pjax.min.js"></script><script>let pjaxSelectors = [
  'title',
  '#config_change',
  '#body-wrap',
  '#rightside-config-hide',
  '#rightside-config-show',
  '.js-pjax'
]

if (false) {
  pjaxSelectors.unshift('meta[property="og:image"]', 'meta[property="og:title"]', 'meta[property="og:url"]')
}

var pjax = new Pjax({
  elements: 'a:not([target="_blank"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  $('script[data-pjax]').each(function () {
    $(this).parent().append($(this).remove())
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof chatBtnFn === 'function' && chatBtnFn()
  typeof panguInit === 'function' && panguInit()

  if (typeof gtag === 'function') {
    gtag('config', '', {'page_path': window.location.pathname});
  }

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // Analytics
  if (false) {
    MtaH5.pgv()
  }

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()

  typeof preloader === 'object' && preloader.endLoading()
})


document.addEventListener('pjax:send', function () {
  typeof preloader === 'object' && preloader.initLoading()
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  $(window).off('scroll')

  //reset readmode
  $('body').hasClass('read-mode') && $('body').removeClass('read-mode')

})</script></div></body></html>